\chapter{Statistics Tool Box}
\label{chap:stat}

The data we collect is stochastic: quantum mechanics is not deterministic, the particle content result of an interaction follows probabilistic laws. Furthermore, experimental effects need to be taken into account. Therefore, a proper statistical treatment is essential to extract quantitative statements from the observed data. This section describes the main statistical procedures that are used to obtain the results described in Chapter \ref{chap:strong_prod} and Chapter \ref{chap:ewk_prod}. The two main topics discussed are parameter estimation (Section \ref{sec:stat:pe}), that has the aim to determine the value of the input parameters that allows to best describe data, and hypothesis testing (Section \ref{sec:stat:ht}), that checks the plausibility of models against the observed data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Parameter Estimation}
\label{sec:stat:pe}

The Likelihood function and the parameter estimation technique based on its maximization are described in this section.

\subsection{The Likelihood Function}

\begin{equation}
\label{eq:stat:exp}
E[n_i] = \mu s_i + b_i \
\end{equation}


\begin{equation}
\label{eq:stat:lik_no_sys}
L(\mu, \vec{\theta}) =
\prod_{j=1}^N \frac{ (\mu s_{j} +
b_{j} )^{n_{j}} }{ n_{j}! }
e^{- (\mu s_{j} + b_{j}) } 
\end{equation}

\subsection{Inclusion of Systematic Uncertainties}

The prediction on the expected number of events is affected by systematic and statistical uncertainties, that are incorporated in the Likelihood in the form of nuisance parameters (NPs).

\subsubsection*{Functional Forms of the Prior}

\iffalse
% this is taken from "Asymptotic formulae for likelihood-based tests of new physics"
\begin{equation}
\label{eq:stat:lik_boh}
L(\mu, \vec{\theta}) =
\prod_{j=1}^N \frac{ (\mu s_{j} +
b_{j} )^{n_{j}} }{ n_{j}! }
e^{- (\mu s_{j} + b_{j}) }   \;\;
\prod_{k=1}^M \frac{ u_k^{m_{k}}} { m_{k}! } \,
e^{- u_k }  \;.
\end{equation}
\fi

\begin{equation}
\label{eq:stat:lik_sys}
L(\mu, \vec{\theta}) =
\prod_{j=1}^N \frac{ (\mu s_{j} +
b_{j} )^{n_{j}} }{ n_{j}! }
e^{- (\mu s_{j} + b_{j}) }   \;\;
\prod_{k=1}^M \rho( \theta_k) \
\end{equation}

\subsubsection*{Profiling}

The post-fit values of the NPs can be compared to the pre-fit values. A central value close to 0 and an uncertainty close to 1 indicates that the fit does not have enough statistical power to profile the uncertainties. If the central values is different from 0, it means that the best value is different from the nominal one; the modified MC will have a better agreement with data than the original one. If the post-fit uncertainty on one NP is smaller than 1, it means that the original assigned uncertainty was too large and the fit was able to constrain the uncertainty on that NP.

\subsubsection*{Interpolation}

\subsection{Profiled Likelihood Ratio}

\begin{equation}
\label{eq:stat:prl}
\prl = \frac{ L(\mu,
\hat{\hat{\theta}}) } {L(\hat{\mu}, \hat{\theta}) } \;.
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hypothesis Testing}
\label{sec:stat:ht}

Hypothesis testing is the statistical procedure that allows to confirm or reject a specific model. In the case of high-energy physics, this allows to discover or exclude a BSM theory. 

\begin{itemize}
\item H$_0$ is the null hypothesis, that corresponds to SM only
\item H$_1$ is the test hypothesis, that corresponds to the SM with the addition of the BSM processes under test
\end{itemize}

The test hypothesis can be generalized by including the signal strength $\mu$, that acts as a cross-section scaling; $\mu$=1 corresponds to the BSM process with its theoretical cross section, while $\mu$=0 is the SM.

\subsection{Test Statistics and p-value}

The test statistics quantifies the compatibility of the observed data with a specific model.
A p-value can be extracted from the distribution of the test statistics according to a certain hypothesis. For each hypothesis under test and given an observation, the p-value corresponds to the probability of having another observation more extreme of the current one.

\iffalse
Points to discuss
- correspondence between p-values and sigma
- arbitrary choice 
\fi 

\begin{equation}
\label{eq:stat:sig}
Z = \Phi^-1(1-p) \
\end{equation}

\subsection{Test Statistics when using PRL}

\begin{equation}
\label{eq:stat:tmu}
t_{\mu} = -2 \log \prl
\end{equation}


\begin{equation}
\label{eq:stat:pmu}
p_{\mu} = \int_{t_{\mu,{\rm obs}}}^{\infty} f(t_{\mu} | \mu ) \,
d t_{\mu} \;,
\end{equation}

\subsubsection*{Test Statistics for Exclusion}

\begin{equation}
\label{eq:qmu}
q_{\mu} =
\left\{ \! \! \begin{array}{ll}
               - 2 \ln \lambda(\mu)  & \hat{\mu} \le \mu  \;, \\*[0.2 cm]
               0 & \hat{\mu} > \mu \;,
              \end{array}
       \right.
\end{equation}

\subsubsection*{Test Statistic for Discovery}

\begin{equation}
\label{eq:stat:q0}
q_{0} =
\left\{ \! \! \begin{array}{ll}
               - 2 \ln \lambda(0)
               & \quad \hat{\mu} \ge 0 \;, \\*[0.3 cm]
               0 & \quad \hat{\mu} < 0  \;,
              \end{array}
       \right.
\end{equation}

\begin{equation}
\label{eq:stat:p0}
p_{0} = \int_{q_{0,{\rm obs}}}^{\infty} f(q_{0} | 0 ) \, d q_{0} \;.
\end{equation}

\subsubsection*{Allow Only Positive Signals}

\begin{equation}
\label{eq:stat:lik:excl}
\tilde{\lambda}({\mu}) =
\left\{ \! \! \begin{array}{ll}
               \frac{ L(\mu,
               \hat{\hat{\vec{\theta}}}(\mu)) }
               {L(\hat{\mu}, \hat{\vec{\theta}}) }
                 & \hat{\mu} \ge 0 , \\*[0.3 cm]
                \frac{ L(\mu,
               \hat{\hat{\vec{\theta}}}(\mu)) }
               {L(0, \hat{\hat{\vec{\theta}}}(0)) }
 & \hat{\mu} < 0 \;.
              \end{array}
       \right.
\end{equation}

\iffalse
% this equation in the original paper uses the notation t and not q
\begin{equation}
\label{eq:stat:q:excl}
\qmu = - 2 \ln \tilde{\lambda}(\mu) =
\left\{ \! \! \begin{array}{ll}
               - 2 \ln \frac{L(\mu, \hat{\hat{\vec{\theta}}}(\mu))}
                {L(0, \hat{\hat{\theta}}(0))}
                & \quad \hat{\mu} < 0  \;, \\*[0.2 cm]
               -2 \ln \frac{L(\mu, \hat{\hat{\vec{\theta}}}(\mu))}
                {L(\hat{\mu}, \hat{\vec{\theta}})}
&  \quad \hat{\mu} \ge 0  \;.
              \end{array}
       \right.
\end{equation}
\fi

\subsection{The CLs Method}

The \cls method \cite{JUNK1999435}

\begin{equation}
\cls = \dfrac{\pmu}{1 - \pzero}
\end{equation}

\subsection{Distribution of the Test Statistic}

\subsubsection*{Pseudo-experiments}

\subsubsection*{Asymptotic Approximation}
Asymptotic approximation \cite{Cowan2011}

\noindent Wald's approximation \cite{Wald1943}

\begin{equation}
\label{eq:wald}
t_{\mu} = -2 \ln \lambda(\mu)
= \frac{(\mu - \hat{\mu})^2}{\sigma^2} + {\cal  O}(1/\sqrt{N}) \;.
\end{equation}

 $t_{\mu} = -2 \ln \lambda(\mu)$ is distributed according to a noncentral chi-square distribution with one degree of freedom:

\begin{equation}
\label{eq:stat:ftmulambda}
f(t_{\mu};\Lambda) = \frac{1}{2 \sqrt{t_{\mu}}} \frac{1}{\sqrt{2 \pi}}
\left[ \exp \left( - \frac{1}{2}
\left( \sqrt{t_{\mu}} + \sqrt{\Lambda} \right)^2 \right) +
\exp \left( - \frac{1}{2} \left( \sqrt{t_{\mu}} - \sqrt{\Lambda} \right)^2
\right) \right] \;,
\end{equation}

\noindent where the noncentrality parameter $\Lambda$ is

\begin{equation}
\label{eq:stat:noncentrality}
\Lambda = \frac{(\mu - \mu^{\prime})^2}{\sigma^2} \;.
\end{equation}
